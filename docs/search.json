[
  {
    "objectID": "Airline Customer Reviews Analysis.html",
    "href": "Airline Customer Reviews Analysis.html",
    "title": "Airline Reviews Analysis",
    "section": "",
    "text": "In this project, I will apply text and sentiment analysis to explore the frequency of words, trends, and create visualizations on a dataset of airline customer reviews. The dataset is found on Kaggle website that includes airline reviews from 2006 to 2019 for popular international and domestic airlines around the world. According to the author, the data is scraped in Spring 2019."
  },
  {
    "objectID": "Airline Customer Reviews Analysis.html#bing-net-sentiment",
    "href": "Airline Customer Reviews Analysis.html#bing-net-sentiment",
    "title": "Airline Reviews Analysis",
    "section": "Bing Net Sentiment",
    "text": "Bing Net Sentiment\nBesides exploring the negative and positive sentiments as two different groups, we can also add them together to get a net sentiment. Since the number of total words heavily depends on the number of total reviews for each airline, I first calculated the net bing sentiment for each airline, divided the net sentiment by the number of review words in each airline, and selected the top five airlines with the highest proportion.\n\nairline_counts &lt;- tidy_airline |&gt;\n  group_by(airline) |&gt;\n  summarise(n = n()) |&gt;\n  rename(tot_word = n)\n\n\n# highest net sentiment\ntidy_airline |&gt;\n  inner_join(bing_sentiments) |&gt;\n  count(airline, sentiment, sort = TRUE) |&gt;\n  group_by(airline, sentiment) |&gt;\n  slice_max(n, n = 1) |&gt;\n  ungroup() |&gt;\n  pivot_wider(names_from = sentiment, values_from = n) |&gt;\n  inner_join(airline_counts) |&gt;\n  mutate(sentiment = positive - negative,\n         prop_sentiment = sentiment/tot_word)|&gt;\n  slice_max(sentiment, n = 5) |&gt;\n  ggplot(aes(fct_reorder(airline, prop_sentiment), \n             prop_sentiment, fill = airline)) +\n  \n  geom_col() +\n  coord_flip() +\n  labs(title = \"Top 5 Airlines with Highest Proportion Net Bing Sentiment\",\n       x = \"Airlines\",\n       y = \"Proportion of Net Sentiment\",\n       fill = \"Airline\") +\n  theme_minimal() +\n  scale_fill_colorblind()\n\n\n\n\n\n\n\n\nWee see that the top 5 airlines with the highest proportion are China Southern Airlines, Qatar Airways, Singapore, Lufthansa, and Cathay Pacific Airways. Take a closer look and we see that these airlines are known as some of the best airlines in the world (https://www.worldairlineawards.com/worlds-top-100-airlines-2025/). We can also see that none of the American airlines made it to top five even though they are in the top five of word counts. So let us take a deeper dive into the US airlines to see what the customers are saying!"
  },
  {
    "objectID": "Airline Customer Reviews Analysis.html#explore-us-airlines",
    "href": "Airline Customer Reviews Analysis.html#explore-us-airlines",
    "title": "Airline Reviews Analysis",
    "section": "Explore US Airlines",
    "text": "Explore US Airlines\nIn this section, I used nrc sentiments to explore review sentiments since nrc sentiments give us more emotions that we can get more meanings from. I have removed “positive” and “negative” sentiments because I would like to really focus on the actual emotions.\n\nus_airline &lt;- small_airline |&gt;\n  filter(airline %in% c(\"American Airlines\", \"Alaska Airlines\", \n                        \"Jetblue Airways\", \"Southwest Airlines\", \n                        \"Delta Air Lines\", \"United Airlines\", \n                        \"Spirit Airlines\", \"Frontier Airlines\")) |&gt;\n  select(airline) |&gt; distinct(airline)\n\n\nnrc_sentiments &lt;- get_sentiments(\"nrc\")\ntidy_airline |&gt;\n  inner_join(us_airline) |&gt;\n  anti_join(smart_stopwords) |&gt;\n  inner_join(nrc_sentiments) |&gt;\n  filter(sentiment != \"positive\" & sentiment != \"negative\") |&gt;\n  count(airline, sentiment) |&gt;\n  left_join(airline_counts) |&gt;\n  mutate(airline = fct_relevel(airline, \"United Airlines\", \n                               \"American Airlines\", \"Spirit Airlines\",\n                               \"Delta Air Lines\", \"Frontier Airlines\",\n                               \"Southwest Airlines\", \"Jetblue Airways\",\n                               \"Alaska Airlines\"\n                               ),\n         prop_sentiment = n/tot_word\n         ) |&gt;\n  ggplot(aes(fct_reorder(sentiment, prop_sentiment), \n             prop_sentiment, fill = sentiment) ) +\n  geom_col() +\n  facet_wrap(~airline, ncol = 2) +\n  labs(title = \"NRC Sentiment Across US Airlines\",\n       x = \"Sentiment\",\n       y = \"Sentiment Proportion\",\n       fill = \"Sentiment\") +\n  theme(axis.text.x = element_blank()) +\n  theme_minimal() +\n  scale_fill_colorblind() +\n  coord_flip()\n\n\n\n\n\n\n\n\nUsing the nrc sentiment, we see that “trust” sentiment has the highest proportion in all airlines except Frontier Airlines with “anticipation” having just a slightly higher proportion than “trust” has. In some airlines such as American Airlines, Frontier Airlines, and Jetblue Airways, “sadness” has a little bit more counts than “joy” has."
  },
  {
    "objectID": "Asian American QOL.html",
    "href": "Asian American QOL.html",
    "title": "An Overview of Asian-American Quality-of-Life Predictors",
    "section": "",
    "text": "Introduction\n      As Asian-Americans are considered the fastest growing minority group in the U.S, and emerge as the largest group of new immigration (Cohn, 2015), understanding the progress, setbacks, and their current status might better explain trends within the Asian-American demographic and predict future immigration trends in the U.S. This is especially pertinent as they are one of the most understudied minority groups (Ðoàn et al., 2019). Therefore, we would like our research to answer the question: What is the role of age, income, mental health, length of residency, and English skill in predicting our response variable of interest: Quality of Life (QoL). Our null hypothesis is that these predictors are equal to zero. Our alternative hypothesis suggests that at least one of these four predictors is not equal to zero.\n      Literature on this topic provides a diverse list of factors that influence Asian-American Quality of Life (AAQoL). A Pew Research Center study found that the majority of Asian immigrants in the U.S. (77%) believe that their standard of living is higher than their parents’, but under half (48%) think their children’s standard of living will be better than theirs (Tian et al., 2024). For U.S.-born Asian adults, these proportions drop to 60% and 29%, respectively. (Tian et al., 2024). This illustrates the importance of birthplace in influencing quality of life. In Hawaii, studies have found that multi-ethnic Asian-Americans reported higher satisfaction with both mental and physical health compared to their mono-ethnic counterparts in some way (Zhang, 2011). One possible explanation for such a discrepancy is that multi-ethnic individuals have to navigate complex social networks, which bring more social benefits and increased fulfillment. Meanwhile, a different study has found that young Asian-Americans are isolated from their families due to their unmarried status (Jang et al., 2021). Additionally, middle-aged Asian-Americans from 40 to 59 years old have the highest odds of being socially isolated due to their limited English proficiency (Jang et al., 2021). This trend is worth noting as social isolation has been considered a health risk factor comparable to smoking 15 cigarettes a day (Holt-Lunstad et al., 2015). Moreover, limited English proficiency also plays a role in Asian-Americans’ lower participation in health services (Jang & Kim, 2018), which could potentially delay and complicate underlying health conditions, especially with avoidance of treatment and regular check-ups. These references complicate an already-complex argument and will be crucial in providing a more objective and all-encompassing assessment of quality of life for our interpretation.\n.\n\n\nMaterial & Method\n      We utilize a dataset from “The Final Report on the Asian American Quality of Life”, located on the data.austintexas.gov website, the official City of Austin data portal. The report was submitted in October 2016 under the lead of Dr. Yuri Jang from the University of Texas at Austin School of Social Work. This data derives from large-scale paper surveys distributed to Asian-American residents of Austin, Texas, through 76 survey sessions across 891 survey sites selected based on the Austin Asian Community Resource Database from August to December, 2015. They include city public centers, educational facilities, medical establishments, religious institutions, local businesses (such as restaurants and grocery stores), and other social services providers. Notably, Asian-Americans are defined in the report as “individuals having origins in any of the original peoples of the Far East, Southeast Asia, or the Indian subcontinent” aged 18 and above. It took about 20 min for each participant to complete the questionnaire, and respondents were each paid U.S. $10 for their participation. The dataset included 2609 unique observations and 231 variables covering many diverse topics. Some are picked from existing national and state surveys, including information on demographics, immigration and acculturation, health, emotional wellbeing, special interests, social and community resources, and general living conditions in Austin. They were collected in Texas as this state displays the highest U.S. growth rate for Asian-American populations, with a 72.4% increase from 2000 to 2010 (City of Austin, Texas, 2018). Austin also currently houses an estimated 110,000 to 115,000 Asian residents, second only to Houston, Texas (City of Austin, Texas, 2018). Our dataset highlights an ever-increasing population, and while our findings may not apply to other states with slower growth rates, they will prove pertinent in recognizing patterns, as the proportion of Asian-Americans in the United States is projected to increase from 5.6% to 10% by 2050 (City of Austin, Texas, 2018).\n      We are dealing with a relatively robust dataset that has been well-constructed. As a result, the data was mess-free, and we did not need to clean anything. Our only problem was the presence of NAs in our observed variables. Due to our large sample size of 2565 observations in total after removing 267 observations that contain NAs from our variables of interest, we decided it was appropriate to simply remove them, as we have a sufficient number of datapoints left. Our summary statistics can be seen in the tables below. Table 1 focuses on categorical variables by showing the proportion of the levels per variable. For example, household income in USD annually has been divided into 2 groups: under $70,000 and above that, as 42% of the sample have income of over $70,000. Mental health is also reported with proportions, including 5 groupings from ‘Poor’ to ‘Excellent’. On the other hand, our numerical summary statistic is calculated in Table 2 with the mean and standard deviation, plus the number of missing values (NAs) we removed from the dataset per variable (column).\n      Since quality of life is a numeric response variable, and there are multiple predictors presented, we believe it is reasonable to use the Multiple Linear Regression (MLR) model. Initially, we composed a full model with 12 variables as shown in Table 1, and built an MLR model utilizing all of them. Although this model explained the most variability in the dataset (Adjusted R-squared = 0.285), there were concerns about multicollinearity. To achieve a simpler, more straightforward model in mind, we first selected the best suited three subset that do not contain confounding variables based on significance (by a two-sided t-test) and relevancy (regarding the context of the survey). Then, limit them to only one promising subset including ‘Age’, ‘Income’, ‘Present Mental Health’, ‘Duration of Residency’, and ‘English Difficulties’. However, we suspected that ‘Income’ might not be as significant in the subset, so a nested F-test was performed for a model with ‘Income’ and one without. Consequently, our test results indicated that the model with ‘Income’ is statistically significant (p &lt; 0.001). Therefore, we decided to examine quality of life with age, income, duration of residency, present mental health, and English speaking difficulties as the final predictors. In our initial Exploratory Data Analysis, the QoL score is skewed left, but since we wanted to make it skewed right and take a log transformation, we decided to transform QoL score and create a new variable called \\(log_{quality}\\) which can be calculated using this following formula: \\(log_{quality} = log(11-QoL)\\). After finding the estimated coefficients from the MLR model, we applied the formula \\(1/(e^{-estimate})\\) to retransform the QoL scores.\n\n\nResults\n\n\n\n\nTable 1: Categorical Variables Summary Statistics\n\n\n\n\n\n\n\nVariables\nMissing values\nN = 2,5651\n\n\n\n\nEthnicity\n0\n\n\n\n\n    Asian Indian\n\n\n568 (22%)\n\n\n    Chinese\n\n\n630 (25%)\n\n\n    Filipino\n\n\n262 (10%)\n\n\n    Korean\n\n\n467 (18%)\n\n\n    Other\n\n\n143 (5.6%)\n\n\n    Vietnamese\n\n\n495 (19%)\n\n\nMarital Status\n13\n\n\n\n\n    Living with a partner\n\n\n102 (4.0%)\n\n\n    Married\n\n\n1,704 (67%)\n\n\n    Other\n\n\n29 (1.1%)\n\n\n    Single\n\n\n717 (28%)\n\n\nIncome (US dollars)\n187\n\n\n\n\n    under70k\n\n\n1,390 (58%)\n\n\n    over70k\n\n\n988 (42%)\n\n\nPresent Mental Health\n9\n\n\n\n\n    Excellent\n\n\n630 (25%)\n\n\n    Fair\n\n\n190 (7.4%)\n\n\n    Good\n\n\n720 (28%)\n\n\n    Poor\n\n\n28 (1.1%)\n\n\n    Very Good\n\n\n988 (39%)\n\n\nEnglish Difficulties\n31\n\n\n\n\n    Much\n\n\n541 (21%)\n\n\n    Not at all\n\n\n765 (30%)\n\n\n    Not much\n\n\n723 (29%)\n\n\n    Very much\n\n\n505 (20%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\n\nAs shown in Figure 1(a), there are five levels of Present Mental Health indicate the self-reported response of participants about their state of mental health. The median of the QoL score increases as the positivity of the levels increases. In Figure 1(b), there are four levels of English Difficulties. Participants in lower levels of English Difficulties (“Not at all”, “Not much”) have higher median QoL score than level “Much”. However, level “Very much” has higher median QoL score than that of level Much. In Figure 1(c), there are two levels of Income Groups with level “under70k” a collapse of seven levels of income range. The medians of two groups are the same at 8, but level ‘under70k’ has a larger IQR than that of level ‘over70k’\n\n\n\n\n\n\nTable 2: Numerical Variables Summary Statistics\n\n\nVariables\nMissing values\nN = 2,5651\n\n\n\n\nQuality of life (Scale of 1-10)\n0\n7.67 ± 1.63\n\n\nAge (years)\n4\n43 ± 17\n\n\nHousehold size (persons)\n11\n3.29 ± 1.47\n\n\nDuration of residency (years)\n36\n16 ± 13\n\n\nEducation completed (years)\n31\n15.11 ± 2.41\n\n\n\n1 Mean ± SD\n\n\n\n\n\n\n\n[1] “kableExtra” “knitr_kable”\n\nTable 3: Predictors of Quality of Life (Transformed)\n\n\n\n\n\n\n\n\n\n\n\nMLR Model (Transformed)\n\n\n\n\nCoefficients\np-value\nLower Bound\nUpper Bound\n\n\n\n\nAge\n0.998**\n0.007\n0.997\n0.999\n\n\nIncome over $70,000\n1.189***\n0\n1.143\n1.236\n\n\nMental health (Fair)\n0.516***\n0\n0.474\n0.561\n\n\nMental health (Good)\n0.62***\n0\n0.587\n0.654\n\n\nMental health (Poor)\n0.519***\n0\n0.43\n0.626\n\n\nMental health (Very Good)\n0.739***\n0\n0.704\n0.776\n\n\nDuration of residency\n1.006***\n0\n1.004\n1.008\n\n\nEnglish difficulties (Not at all)\n1.167***\n0\n1.101\n1.236\n\n\nEnglish difficulties (Not much)\n1.011\n0.696\n0.957\n1.068\n\n\nEnglish difficulties (Very much)\n1.05\n0.115\n0.988\n1.116\n\n\nNum.Obs.\n2258\n\n\n\n\n\nR2 Adj.\n0.265\n\n\n\n\n\n\n      In general, our data shows many significant coefficients with each of the explanatory variables in conjunction with each other (p&lt;0.05). Holding all other variables constant, for each additional year added in age, AAQoL decreases 0.998 (0.997;0.999) times on average, while for each increment of a year of residency, AAQoL increases 1.006 (1.004;1.008) times. Our categorical variables show a similar picture with an overall positive correlation with QoL. Particularly with other variables accounted for, Asian-American whose annual household income from $70,000 and above have 1.189(1.143;1.236) times higher QoL compared to the lowest income group of $0 to $10000 on average. It is also worth noting that Asian-Americans with a “Very Good” mental health state have a QoL that is 0.739 (0.704;0.776) times lower than the people with “Excellent” mental health. Additionally, the QoL of Asian-Americans who considered their English speaking difficulty as “Not at all” was 1.167 (1.101;1.236) times larger than those who reported their difficulty as “Much”, in respect to other variables.\n\n\nDiscussion\n      The above interpretation of our coefficients is within our expectation based on established trends of higher income, better mental health, lengthening of stay and more cultural assimilation correlate to a higher degree of life satisfaction for everyone, including Asian-Americans. While aging demonstrated a negative correlation to AAQoL, the difference is not really large, and thus less meaningful when compared to the rest of the explanatory variables. Since age is in opposition with duration of residency, which means that the longer people live in a place, they have higher life satisfaction as they get used to the new location but might have lower life satisfaction due to aging, making these two variable coefficients cancel out. Our final model does not contain any confounding variables as we have eliminated them beforehand.\n      Contextually, we do not have a chance to compare variables that are related to our literature like ethnicity, marital status or household size in our final model. Thus, the relationship of the predictors we have might be strictly interpreted for correlation and not for causal relationship. While all states of mental health are related to QoL in our model, it is interesting that the supposedly worst mental health state of “Poor” has a smaller difference to “Excellent” mental health. These small discrepancy could be accounted for by the noteworthy overlapping between “Poor” state of mental health’s QoL scores to “Fair” and “Good” mental health states, with a small QoL score overlap to “Excellent” and “Very Good” in the Present Mental Health boxplot in Figure 1. The AAQoL report has noted that there is a possible lack of awareness of mental health problems among the populations as more than one third survey participants believed that depression is a sign of weakness, and almost one half have misconception of antidepressant, they think that it is addictive. The internalization of the model minority myth that all Asian-American is wealthy, highly educated and problem free (Yi et al., 2016) could exacerbate the predisposing mental health stigma of Asian-Americans which stem from Asian cultural emphasis on Asian cultural values like emotional restrain, shame avoidance, and saving face (Shea & Yeh, 2008). These results are making the case for a nuanced approach of providing Asian-Americans, especially immigrants the anticipated help they need.\n      However, one limitation from our model is variability with the income groups, where the over $70,000 group is disproportionately represented of almost half the sample. Self-assessed mental health might be underestimated due to stigma while English difficulties could be overestimated from actual capability. While Austin serves as an excellent sample of the Asian population’s life experience, it might overrepresent the new immigration population with much higher income or average income of the American middle-class. Future study could further survey other cities in different states, such as California that also experience an increase of Asian immigration and also expand our research to different racial groups and ethnicities. The same AAQoL survey could also be re-introduced as it has been almost 10 years since this report and new insight could be made between the first and the second future report.\n\n\n\nBibliography\nCity of Austin, Texas. (2018). Final Report of the Asian American Quality of Life (AAQoL). In Austintexas.gov. https://data.austintexas.gov/dataset/Final-Report-of-the-Asian-American-Quality-of-Life/hc5t-p62z/about_data\nCohn, D. (2015, October 5). Future immigration will change the face of America by 2065. Pew Research Center. https://www.pewresearch.org/short-reads/2015/10/05/future-immigration-will-change-the-face-of-america-by-2065/\nÐoàn, L. N., Takata, Y., Sakuma, K.-L. K., & Irvin, V. L. (2019). Trends in Clinical Research Including Asian American, Native Hawaiian, and Pacific Islander Participants Funded by the US National Institutes of Health, 1992 to 2018. JAMA Network Open, 2(7), e197432–e197432. https://doi.org/10.1001/jamanetworkopen.2019.7432\nHolt-Lunstad, J., Smith, T. B., Baker, M., Harris, T., & Stephenson, D. (2015). Loneliness and Social Isolation as Risk Factors for Mortality. Perspectives on Psychological Science, 10(2), 227–237. https://doi.org/10.1177/1745691614568352\nJang, Y., & Kim, M. T. (2018). Limited English Proficiency and Health Service Use in Asian Americans. Journal of Immigrant and Minority Health, 21(2), 264–270. https://doi.org/10.1007/s10903-018-0763-0\nJang, Y., Park, J., Choi, E. Y., Cho, Y. J., Park, N. S., & Chiriboga, D. A. (2021). Social isolation in Asian Americans: risks associated with socio-demographic, health, and immigration factors. Ethnicity and Health, 27(6), 1428–1441. https://doi.org/10.1080/13557858.2021.1881765\nPeterson Foundation. (2025, March 12). Income and Wealth in the United States: An Overview of Recent Data. Peterson Foundation. https://www.pgpf.org/article/income-and-wealth-in-the-united-states-an-overview-of-recent-data/\nShea, M., & Yeh, C. (2008). Asian American Students’ Cultural Values, Stigma, and Relational Self-construal: Correlates of Attitudes Toward Professional Help Seeking. Journal of Mental Health Counseling, 30(2), 157–172. https://doi.org/10.17744/mehc.30.2.g662g5l2r1352198\nTian, Z., Im, C., Mukherjee, S., & Budiman, A. (2024, October 9). 2. Asian American immigrants’ views of quality of life in the U.S. Pew Research Center. https://www.pewresearch.org/race-and-ethnicity/2024/10/09/asian-american-immigrants-views-of-quality-of-life-in-the-u-s/\nYi, S. S., Kwon, S. C., Sacks, R., & Trinh-Shevrin, C. (2016). Commentary: Persistence and Health-Related Consequences of the Model Minority Stereotype for Asian Americans. Ethnicity & Disease, 26(1), 133. https://doi.org/10.18865/ed.26.1.133\nZhang, W. (2011). Health Disparities and Relational Well-Being between Multi- and Mono-Ethnic Asian Americans. Social Indicators Research, 110(2), 735–750. https://doi.org/10.1007/s11205-011-9956-9"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nhi Luong",
    "section": "",
    "text": "Hello! I’m Nhi. I’m an undergraduate student studying Mathematics, Data Science, and Chinese at St. Olaf College. I love learning about text and sentiment analysis and predictive models for transportation-related problems, especially in aviation. I collect a lot of Lego sets and my favorite themes are Architecture and Technic. Explore around my website to learn more!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Nhi Luong",
    "section": "",
    "text": "Hello! I’m Nhi. I’m an undergraduate student studying Mathematics, Data Science, and Chinese at St. Olaf College. I love learning about text and sentiment analysis and predictive models for transportation-related problems, especially in aviation. I collect a lot of Lego sets and my favorite themes are Architecture and Technic. Explore around my website to learn more!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Nhi Luong",
    "section": "Education",
    "text": "Education\nSt. Olaf College | Northfield, MN\nB.A. in Mathematics, Chinese, Asian Studies\nConcentration in Statistics & Data Science"
  },
  {
    "objectID": "R_tip.html#section",
    "href": "R_tip.html#section",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Let’s explore tmcn!"
  },
  {
    "objectID": "R_tip.html#short-introduction",
    "href": "R_tip.html#short-introduction",
    "title": "R Tip of the Day",
    "section": "Short Introduction",
    "text": "Short Introduction\n\nThe tmcn package is a text mining toolkit for the Chinese language.\n\nWe can use functions from the package to:\n\n\nConvert from Traditional Chinese to Simplified Chinese (and reverse)\nConvert Chinese text to pinyin format\nOutput dictionary of Chinese stop words\nGive some useful information about a character such as pinyin, radicals, stroke number of radical"
  },
  {
    "objectID": "R_tip.html#convert-chinese-text-to-pinyin-topinyin",
    "href": "R_tip.html#convert-chinese-text-to-pinyin-topinyin",
    "title": "R Tip of the Day",
    "section": "Convert Chinese text to Pinyin: toPinyin()",
    "text": "Convert Chinese text to Pinyin: toPinyin()\n\n\nPinyin is a romanized spelling of Chinese words. For example:\n\n\n\n\n妈妈 –&gt; ma1ma1\n\n\n\n\n爸爸 –&gt; ba4ba"
  },
  {
    "objectID": "R_tip.html#lets-try-it",
    "href": "R_tip.html#lets-try-it",
    "title": "R Tip of the Day",
    "section": "Let’s try it!",
    "text": "Let’s try it!\n\nchinese_words &lt;- c(\"爸爸\",\"妈妈\",\"圣奥拉夫\",\"老师\",\"学生\",\"电脑\",\"大学\",\"秋季\")"
  },
  {
    "objectID": "R_tip.html#lets-try-it-1",
    "href": "R_tip.html#lets-try-it-1",
    "title": "R Tip of the Day",
    "section": "Let’s try it!",
    "text": "Let’s try it!\n\nchinese_words &lt;- c(\"爸爸\",\"妈妈\",\"圣奥拉夫\",\"老师\",\"学生\",\"电脑\",\"大学\",\"秋季\")\ntoPinyin(chinese_words)\n\n[1] \"baba\"        \"mama\"        \"shengaolafu\" \"laoshi\"      \"xuesheng\"   \n[6] \"diannao\"     \"daxue\"       \"qiuji\""
  },
  {
    "objectID": "R_tip.html#how-about-a-table",
    "href": "R_tip.html#how-about-a-table",
    "title": "R Tip of the Day",
    "section": "How about a table?",
    "text": "How about a table?\n\n\n\n\n\n\n\nChinese Words\nPinyin\nEnglish Translation\n\n\n\n\n爸爸\nbaba\nfather\n\n\n妈妈\nmama\nmother\n\n\n圣奥拉夫\nshengaolafu\nSaint Olaf\n\n\n老师\nlaoshi\nteacher\n\n\n学生\nxuesheng\nstudent\n\n\n电脑\ndiannao\ncomputer\n\n\n大学\ndaxue\nuniversity\n\n\n秋季\nqiuji\nfall (season)"
  },
  {
    "objectID": "R_tip.html#convert-traditional-to-simplified-totrad",
    "href": "R_tip.html#convert-traditional-to-simplified-totrad",
    "title": "R Tip of the Day",
    "section": "Convert traditional to simplified: toTrad()",
    "text": "Convert traditional to simplified: toTrad()\n\nchinese_words &lt;-c(\"爸爸\",\"妈妈\",\"圣奥拉夫\",\"老师\",\"学生\",\"电脑\",\"大学\",\"秋季\")"
  },
  {
    "objectID": "R_tip.html#convert-traditional-to-simplified-totrad-1",
    "href": "R_tip.html#convert-traditional-to-simplified-totrad-1",
    "title": "R Tip of the Day",
    "section": "Convert traditional to simplified: toTrad()",
    "text": "Convert traditional to simplified: toTrad()\n\nchinese_words &lt;-c(\"爸爸\",\"妈妈\",\"圣奥拉夫\",\"老师\",\"学生\",\"电脑\",\"大学\",\"秋季\")\ntoTrad(chinese_words)\n\n[1] \"爸爸\"     \"媽媽\"     \"聖奧拉夫\" \"老師\"     \"學生\"     \"電腦\"     \"大學\"    \n[8] \"秋季\"    \n\n\n\n\n\n\n\n\n\nSimplified\nTraditional\nEnglish Translation\n\n\n\n\n妈妈\n媽媽\nmother\n\n\n电脑\n電腦\ncomputer"
  },
  {
    "objectID": "R_tip.html#explore-gbk-dataset",
    "href": "R_tip.html#explore-gbk-dataset",
    "title": "R Tip of the Day",
    "section": "Explore GBK Dataset",
    "text": "Explore GBK Dataset\n\nGBK dataset provides users some useful information of a character such as pinyin, radical, stroke numbers of radical.\n\n\nGBK |&gt;\n  as_tibble() |&gt;\n  slice_head(n = 3)\n\n# A tibble: 3 × 8\n  GBK   py0   py        Radical Stroke_Num_Radical Stroke_Order Structure   Freq\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;\n1 吖    a     ā yā      口                       3 丨フ一丶ノ丨 左右          26\n2 阿    a     ā ɑ ē     阝                       2 フ丨一丨フ一丨…… 左右      526031\n3 啊    a     ɑ á à ǎ ā 口                       3 丨フ一フ丨一丨フ一丨…… 左中右     53936"
  },
  {
    "objectID": "R_tip.html#explore-gbk-dataset-1",
    "href": "R_tip.html#explore-gbk-dataset-1",
    "title": "R Tip of the Day",
    "section": "Explore GBK Dataset",
    "text": "Explore GBK Dataset\nchinese_words_split &lt;- c(\"爸\",\"爸\",\"妈\",\"妈\",\"圣\",\"奥\",\"拉\",\"夫\",\"老\",\"师\",\"学\",\"生\",\"电\",\"脑\",\"大\",\"学\",\"秋\",\"季\")\nchinese_words_split |&gt;\n  as.tibble() |&gt;\n  distinct(value) |&gt;\n  inner_join(GBK, join_by(value == GBK)) |&gt;\n  slice_head(n = 5) |&gt;\n  select(1:5)"
  },
  {
    "objectID": "R_tip.html#explore-gbk-dataset-2",
    "href": "R_tip.html#explore-gbk-dataset-2",
    "title": "R Tip of the Day",
    "section": "Explore GBK Dataset",
    "text": "Explore GBK Dataset\n\nGBK dataset provides users some useful information of a character such as pinyin, radicals, stroke numbers of radical.\n\n\nchinese_words_split &lt;- c(\"爸\",\"爸\",\"妈\",\"妈\",\"圣\",\"奥\",\"拉\",\"夫\",\"老\",\"师\",\"学\",\"生\",\"电\",\"脑\",\"大\",\"学\",\"秋\",\"季\")\nchinese_words_split |&gt;\n  as.tibble() |&gt;\n  distinct(value) |&gt;\n  inner_join(GBK, join_by(value == GBK)) |&gt;\n  slice_head(n = 5) |&gt;\n  select(1:5)\n\n# A tibble: 5 × 5\n  value py0   py          Radical Stroke_Num_Radical\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;                &lt;int&gt;\n1 爸    ba    bà          父                       4\n2 妈    ma    mā          女                       3\n3 圣    sheng shènɡ       土                       3\n4 奥    ao    ào yù       大                       3\n5 拉    la    lá là lǎ lā 扌                       3"
  },
  {
    "objectID": "R_tip.html#in-action",
    "href": "R_tip.html#in-action",
    "title": "R Tip of the Day",
    "section": "In action!",
    "text": "In action!\n\n“女” + “马” = “妈”"
  },
  {
    "objectID": "R_tip.html#in-action-1",
    "href": "R_tip.html#in-action-1",
    "title": "R Tip of the Day",
    "section": "In action!",
    "text": "In action!\n\n“woman” + “horse” = “mother”\n“女” + “马” = “妈”"
  },
  {
    "objectID": "R_tip.html#journey-to-the-west",
    "href": "R_tip.html#journey-to-the-west",
    "title": "R Tip of the Day",
    "section": "Journey to the West",
    "text": "Journey to the West\n\n\nOne of the most famous Chinese novels is “Journey to the West”. Below is an excerpt from chapter 4.\n\n\njourney_west[[1]][1]\n\n[1] \"話表齊天大聖到底是個妖猴，更不知官銜品從，也不較俸祿高低，但只註名便 了。那齊天府下二司仙吏，早晚伏侍，只知日食三餐，夜眠一榻，無事牽縈， 自由自在。閑時節會友遊宮，交朋結義。見三清稱個「老」字，逢四帝道個 「陛下」。與那九曜星、五方將、二十八宿、四大天王、十二元辰、五方五老 、普天星相、河漢群神，俱只以弟兄相待，彼此稱呼。今日東遊，明日西蕩， 雲去雲來，行蹤不定。\"\n\n\n\n\n\nLooking at some Chinese stop words from STOPWORDS dataset\n\n\nhead(STOPWORDS)\n\n  word\n1 第二\n2 一番\n3 一直\n4 一个\n5 一些\n6 许多\n\n\n\n\nLink: Journey to the West Novel"
  },
  {
    "objectID": "R_tip.html#can-we-make-a-wordcloud-for-chapter-4",
    "href": "R_tip.html#can-we-make-a-wordcloud-for-chapter-4",
    "title": "R Tip of the Day",
    "section": "Can we make a wordcloud for chapter 4?",
    "text": "Can we make a wordcloud for chapter 4?\n\nCodeWordcloud\n\n\n\njourney_west_token &lt;- journey_west |&gt;\n  unnest_tokens(word, \"第五回 亂蟠桃大聖偷丹　反天宮諸神捉怪\", token = \"words\")\n\njourney_dfs &lt;- journey_west_token |&gt;\n  anti_join(STOPWORDS) |&gt;\n  mutate(simplified = toTrad(word, rev = T)) |&gt;\n  count(simplified, sort = T) |&gt;\n  slice_head(n = 50) |&gt;\n  data.frame()\n\nwordcloud2(\n  journey_dfs, \n  size = 1.2, \n  shape = 'cardioid',\n  minSize = 15\n)"
  },
  {
    "objectID": "Korean Drama Analysis.html",
    "href": "Korean Drama Analysis.html",
    "title": "Korean Drama Analysis",
    "section": "",
    "text": "South Korea’s movies and TV series have played a crucial role in her intensive culture export efforts to boost the nation’s ability to “shape the preferences of others [country] through attraction of values, policies, and culture as distinct from coercion by economic and/or military means” (i.e. soft power) 1. Its influence in the US specifically is undeniable as many Korean cultural products have become a part of the collective mainstream cultural landscape from Kpop music groups like BTS and Black Pink to the critically and commercially acclaimed show: Squid Game. The story of Squid Game focuses on the anxiety and hopelessness of the lower class and neglected groups, whose only option to escape poverty or paying gambling debt is to participate in life-staking children’s games for the entertainment of the ultra-wealthy.\nDive into the same mindset of capitalist horror is the film Parasite which is the first foreign film to win best picture from the Oscar and has been considered a historical step for the Korean media landscape, breaking the old US exclusive-winning convention of this category. These media provide a medium for many during and after the pandemics where the same sentiments of the public resonate with the message of these shows. How do South Korean shows make such an impact? What can we learn from one of the Four Asian Tigers 2 cultural production and publishing’s history and journey? In this project, we want to explore “Which original network has the highest number of Korean drama series on weekdays and weekends, and how does it compare to other days of the week?” and “Which original network has the highest variation in IMDb ratings, and how does it compare to others in terms of the median rating?” .\n\n\nBut how popular is the Korean Wave with its cultural products of television that have taken the world by storm and especially in the West? We would like to touch the surface of the Kdrama popularity through the IMDb rating of these shows while comparing them between networks to assess such impact. Here, we define original network of a show as a broadcasting channel that invests in the show and airs it first.\n\n\n\n\n\nFigure 1: IMDb Ratings distribution by Original Networks for Korean Dramas.This box plot illustrates the distribution of IMDb ratings for Korean dramas produced by 20 original networks. The data reveal that most networks have median ratings between 6 and 8. Netflix stands out with the highest median rating, while Naver TV Cast has the lowest. Outliers are present across several networks, suggesting significant variability in ratings for some shows. The data were collected from mydramalist.com and include IMDb ratings for 810 dramas spanning from 2002 to 2022. The figure provides a descriptive overview of users rating.\n\n\n\n\nObservation\n\nWe explore the IMDb ratings distribution across 21 Korean original networks and see that Netflix has the highest median IMDb ratings, surpassing national public networks such as KBS, SBS, and MBC by half a point. Among these original networks, Netflix is one of a few original network that is a streaming service, but it is doing very well. As Netflix is a global streaming service, this suggests Netflix’s original Korean dramas are doing very well and receiving a lot of attention from global audience. This relates to the concept of soft power we discuss in the introduction where Korean culture and values are getting recognition from the international level.\n\n\n\n\nOne way to examine the strategy to their success would be through the classic strategy of TV networks in their choice of aired day to be competitive with other networks. We will visualized that concept through the top 10 original networks below.\n\n\n\n\n\nFigure 2: Number of Korean Drama Series by Days of the Week Across Top 10 Original Networks. This chart shows that KBS2, SBS, and tvN aired the highest number of Korean drama series, with peaks on Monday, Tuesday, Wednesday, Thursday, and Saturday. Data were collected from the publicly available website mydramalist.com, where users contribute information about Asian TV shows and movies. The dataset includes 810 distinct shows spanning from 2002 to 2022. No statistical tests were performed.\n\n\n\n\nObservation\n\nThis graph illustrates that Tuesday and Wednesday are the most popular days for airing dramas across multiple networks. Conversely, Sundays and Fridays show relatively lower activity in terms of drama releases, indicating a preference for mid-week broadcasting. One possible explanation for these trends due to market demands. Sunday is the final weekend day which leads to lower viewership while the reason Friday has less demand might be due to overtime working/studying schedule or the population at large going out and not staying at home watching TV. Another notable observation would be that the top four networks dominated the market where KBS, MBC and SBS are public television networks and tvN is a private one. Each network also have their own airing strategies as tvN aired the most shows on Saturday, KBS on Monday and Tuesday while MBC have the highest proportion on Wednesday and Thursday. We can also observed that investment in the public sector (KBS, SBS) is as important as the private sector (tvN) for the television industry as the government heavily subsided the industry through investment in film education from the Korean Film Academy, interest-free or low-interest loans for enterprise 3 and tax policies4 that encourage local business to invest in film production beside their own direct investment to the industry 5.\n\n\n\n\nOn March 11, 2020, WHO declared COVID-19 outbreak a global pandemic 6 that continued to last for the next 3 years. Following the announcement was a string of global and national shutdowns. Because of that, more people spent more time inside watching shows on Netflix and got to learn more about Korean drama. In this section, we would like to look at Korean drama series on Netflix to identify changes in the number of drama series as well as the most popular genres among the series before and after the Covid pandemic. We have decided to use “2020” as a mark for before and after Covid pandemic instead of “March 2020” for simplicity.\n\n\n\n\n\nFigure 3: The number of Korean drama series on Netflix after 2020 is two times greater than the number of Korean shows on Netflix before 2020. The data were collected from mydramalist.com and include IMDb ratings for 810 dramas spanning from 2002 to 2022. The years were grouped into two categorical groups for comparison. No statistical tests were performed.\n\n\n\n\n\nWe briefly look at the number of Korean drama shows on Netflix before 2020 and after 2020 and see that after the announcement of the COVID-19 a global pandemic up until 2022, Netflix has added 36 more Korean drama shows into their collection, showing Netflix’s interest in investing into Korean shows in these recent years. Naturally, our next question would be “Then does Netflix have a preference of Korean drama shows’ genres that they want to add that can attract more viewers?” We look at this question in the following graph.\n\n\n\n\n\n\nFigure 3: Top 5 Korean Drama Genres on Netflix from 2016 to 2022. The red vertical line marks the transition year 2020. Shows with omance genres exhibited the fastest growth, peaking at around 155 in 2020. Data were sourced from the publicly available website mydramalist.com, where users contribute information about Asian TV shows and movies. No statistical tests were conducted.\n\n\n\n\nObservation\n\nThis graph indicates a shift in focus toward action and romance genres while reducing investment in thriller and comedy content from Netflix in Kdrama. We can see that romance dramas experienced a sharp increase, peaking around 2020, followed by a slight decline but maintaining it’s top position even in 2022. The action genres initially grew at a slower pace but also grew significantly after 2020. Mystery and thriller genres followed similar trajectories, with significant activity between 2018 and 2020 but a noticeable decline after 2021. Comedy dramas showed a slower growing trend which peaked in 2019 and then declined gradually. The red vertical line marking 2020 reflects the impact of a pandemic which might change production priorities and viewer preferences."
  },
  {
    "objectID": "Korean Drama Analysis.html#context-and-background",
    "href": "Korean Drama Analysis.html#context-and-background",
    "title": "Korean Drama Analysis",
    "section": "",
    "text": "South Korea’s movies and TV series have played a crucial role in her intensive culture export efforts to boost the nation’s ability to “shape the preferences of others [country] through attraction of values, policies, and culture as distinct from coercion by economic and/or military means” (i.e. soft power) 1. Its influence in the US specifically is undeniable as many Korean cultural products have become a part of the collective mainstream cultural landscape from Kpop music groups like BTS and Black Pink to the critically and commercially acclaimed show: Squid Game. The story of Squid Game focuses on the anxiety and hopelessness of the lower class and neglected groups, whose only option to escape poverty or paying gambling debt is to participate in life-staking children’s games for the entertainment of the ultra-wealthy.\nDive into the same mindset of capitalist horror is the film Parasite which is the first foreign film to win best picture from the Oscar and has been considered a historical step for the Korean media landscape, breaking the old US exclusive-winning convention of this category. These media provide a medium for many during and after the pandemics where the same sentiments of the public resonate with the message of these shows. How do South Korean shows make such an impact? What can we learn from one of the Four Asian Tigers 2 cultural production and publishing’s history and journey? In this project, we want to explore “Which original network has the highest number of Korean drama series on weekdays and weekends, and how does it compare to other days of the week?” and “Which original network has the highest variation in IMDb ratings, and how does it compare to others in terms of the median rating?” .\n\n\nBut how popular is the Korean Wave with its cultural products of television that have taken the world by storm and especially in the West? We would like to touch the surface of the Kdrama popularity through the IMDb rating of these shows while comparing them between networks to assess such impact. Here, we define original network of a show as a broadcasting channel that invests in the show and airs it first.\n\n\n\n\n\nFigure 1: IMDb Ratings distribution by Original Networks for Korean Dramas.This box plot illustrates the distribution of IMDb ratings for Korean dramas produced by 20 original networks. The data reveal that most networks have median ratings between 6 and 8. Netflix stands out with the highest median rating, while Naver TV Cast has the lowest. Outliers are present across several networks, suggesting significant variability in ratings for some shows. The data were collected from mydramalist.com and include IMDb ratings for 810 dramas spanning from 2002 to 2022. The figure provides a descriptive overview of users rating.\n\n\n\n\nObservation\n\nWe explore the IMDb ratings distribution across 21 Korean original networks and see that Netflix has the highest median IMDb ratings, surpassing national public networks such as KBS, SBS, and MBC by half a point. Among these original networks, Netflix is one of a few original network that is a streaming service, but it is doing very well. As Netflix is a global streaming service, this suggests Netflix’s original Korean dramas are doing very well and receiving a lot of attention from global audience. This relates to the concept of soft power we discuss in the introduction where Korean culture and values are getting recognition from the international level.\n\n\n\n\nOne way to examine the strategy to their success would be through the classic strategy of TV networks in their choice of aired day to be competitive with other networks. We will visualized that concept through the top 10 original networks below.\n\n\n\n\n\nFigure 2: Number of Korean Drama Series by Days of the Week Across Top 10 Original Networks. This chart shows that KBS2, SBS, and tvN aired the highest number of Korean drama series, with peaks on Monday, Tuesday, Wednesday, Thursday, and Saturday. Data were collected from the publicly available website mydramalist.com, where users contribute information about Asian TV shows and movies. The dataset includes 810 distinct shows spanning from 2002 to 2022. No statistical tests were performed.\n\n\n\n\nObservation\n\nThis graph illustrates that Tuesday and Wednesday are the most popular days for airing dramas across multiple networks. Conversely, Sundays and Fridays show relatively lower activity in terms of drama releases, indicating a preference for mid-week broadcasting. One possible explanation for these trends due to market demands. Sunday is the final weekend day which leads to lower viewership while the reason Friday has less demand might be due to overtime working/studying schedule or the population at large going out and not staying at home watching TV. Another notable observation would be that the top four networks dominated the market where KBS, MBC and SBS are public television networks and tvN is a private one. Each network also have their own airing strategies as tvN aired the most shows on Saturday, KBS on Monday and Tuesday while MBC have the highest proportion on Wednesday and Thursday. We can also observed that investment in the public sector (KBS, SBS) is as important as the private sector (tvN) for the television industry as the government heavily subsided the industry through investment in film education from the Korean Film Academy, interest-free or low-interest loans for enterprise 3 and tax policies4 that encourage local business to invest in film production beside their own direct investment to the industry 5.\n\n\n\n\nOn March 11, 2020, WHO declared COVID-19 outbreak a global pandemic 6 that continued to last for the next 3 years. Following the announcement was a string of global and national shutdowns. Because of that, more people spent more time inside watching shows on Netflix and got to learn more about Korean drama. In this section, we would like to look at Korean drama series on Netflix to identify changes in the number of drama series as well as the most popular genres among the series before and after the Covid pandemic. We have decided to use “2020” as a mark for before and after Covid pandemic instead of “March 2020” for simplicity.\n\n\n\n\n\nFigure 3: The number of Korean drama series on Netflix after 2020 is two times greater than the number of Korean shows on Netflix before 2020. The data were collected from mydramalist.com and include IMDb ratings for 810 dramas spanning from 2002 to 2022. The years were grouped into two categorical groups for comparison. No statistical tests were performed.\n\n\n\n\n\nWe briefly look at the number of Korean drama shows on Netflix before 2020 and after 2020 and see that after the announcement of the COVID-19 a global pandemic up until 2022, Netflix has added 36 more Korean drama shows into their collection, showing Netflix’s interest in investing into Korean shows in these recent years. Naturally, our next question would be “Then does Netflix have a preference of Korean drama shows’ genres that they want to add that can attract more viewers?” We look at this question in the following graph.\n\n\n\n\n\n\nFigure 3: Top 5 Korean Drama Genres on Netflix from 2016 to 2022. The red vertical line marks the transition year 2020. Shows with omance genres exhibited the fastest growth, peaking at around 155 in 2020. Data were sourced from the publicly available website mydramalist.com, where users contribute information about Asian TV shows and movies. No statistical tests were conducted.\n\n\n\n\nObservation\n\nThis graph indicates a shift in focus toward action and romance genres while reducing investment in thriller and comedy content from Netflix in Kdrama. We can see that romance dramas experienced a sharp increase, peaking around 2020, followed by a slight decline but maintaining it’s top position even in 2022. The action genres initially grew at a slower pace but also grew significantly after 2020. Mystery and thriller genres followed similar trajectories, with significant activity between 2018 and 2020 but a noticeable decline after 2021. Comedy dramas showed a slower growing trend which peaked in 2019 and then declined gradually. The red vertical line marking 2020 reflects the impact of a pandemic which might change production priorities and viewer preferences."
  },
  {
    "objectID": "Korean Drama Analysis.html#conclusion",
    "href": "Korean Drama Analysis.html#conclusion",
    "title": "Korean Drama Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThrough the project, we have looked at IMDb ratings of Korean shows by original networks, their air day in the week, and changes of Korean shows on Netflix. We see that overall, the Korean drama industry is investing a lot into their shows so that the shows can be known more globally. As a result of their strategy and popularity of Korean drama, these shows receive more investment from Netflix and appear on the streaming service more, which makes a big profit for Netflix while as the same time receive worldwide popularity. We believe that Korean drama shows will continue to thrive even more in the future."
  },
  {
    "objectID": "Korean Drama Analysis.html#footnotes",
    "href": "Korean Drama Analysis.html#footnotes",
    "title": "Korean Drama Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMarklund, Carl. “Soft Power.” International Encyclopedia of Human Geography, edited by Audrey Kobayashi, 2nd ed., Elsevier Science & Technology, 2020. Credo Reference, https://search.credoreference.com/articles/Qm9va0FydGljbGU6NTkwMDQ=?aid=79650.↩︎\nDay, D.-C. “Four Asian Tigers’ Political and Economic Development Revisited 1998-2017: From the Perspective of National Identity”. Asian Journal of Interdisciplinary Research, vol. 4, no. 4, Dec. 2021, pp. 54-61, doi:10.54392/ajir2147.↩︎\nKang, Jennifer M. “Better than Television: The Rise of Korean Web Dramas.” International Journal of Cultural Studies, vol. 24, no. 6, May 2021, p. 136787792110145, https://doi.org/10.1177/13678779211014532.↩︎\nPricewaterhouseCoopers International Limited. “Korea, Republic of - Corporate - Tax Credits and Incentives.” Pwc.com, 2024, taxsummaries.pwc.com/republic-of-korea/corporate/tax-credits-and-incentives. Accessed 12 Dec. 2024.↩︎\nShim, Doobo. “Whiter the Korean Flim Industry?” Acta Koreana, vol. 14, no. 1, June 2011, pp. 213–27, https://doi.org/10.18399/acta.2011.14.1.010. Accessed 1 Apr. 2020.↩︎\nCucinotta, Domenico, and Maurizio Vanelli. “WHO Declares COVID-19 a Pandemic.” Acta Bio Medica: Atenei Parmensis, vol. 91, no. 1, 2020, pp. 157–60. PubMed Central, https://doi.org/10.23750/abm.v91i1.9397.↩︎"
  },
  {
    "objectID": "Digits classification.html",
    "href": "Digits classification.html",
    "title": "Handwritten Digits Classification",
    "section": "",
    "text": "Here we will include all of the functions that we will use in this project.\n\n\nplot_digit &lt;- function(row) {\n  digit_mat &lt;-  row |&gt;\n    select(-digit)|&gt;\n    as.numeric()|&gt;\n    matrix(nrow = 28)\n  \n  image(digit_mat[,28:1])\n}\n\n\nplot_digit(): This function takes in a row of the ‘mnist’ dataset, removes the ‘digit’ column, turns the row into a vector using ‘as.numeric()’ function, and then turns it into a 28 by 28 matrix. Finally, it plots the matrix using the function ‘image()’.\n\n\nplot_region &lt;- function(tbl) {\n  digit_mat &lt;- as.matrix(tbl)*128 # Convert tbl into matrix and assign gray=128\n\n  image(t(digit_mat)[,28:1]) #Plot the image making sure is rotated\n}\n\n#plot_region(region1_class)\n\n\nplot_region(): produce a plot where the pixel of the region of interest is distinguish from the rest of the matrix. It does so through assigning the pixel value of all of the matrix as gray then using a different color for the region of interest. It also make sure that the plot is rotated correctly.\n\n\ncalc_prop &lt;- function (region, row) {\n  # Take row from mnist and transform into a \"digit\" matrix\n  digit_mat &lt;-  row |&gt;\n    as.numeric()|&gt;\n    matrix(nrow = 28) |&gt;\n    t()\n  # Find positions of pixels from \"region\"\n  pos = (region==1)\n  # Subset \"digit\" to the positions and count dark pixels (grey&gt;20)\n  dark = digit_mat[pos]&gt;20 \n  # Return proportion of dark pixels of \"image\" in \"region\"\n  return(sum(dark)/sum(pos))\n}\n\n\nThis function, calc_prop takes a region and a row as arguments and returns the proportion of dark pixels in each region. It does this by taking your dataset as a row and then calculating an proportion of “dark pixels” which it defines as a value of over 20 for grey."
  },
  {
    "objectID": "Digits classification.html#auxiliary-functions",
    "href": "Digits classification.html#auxiliary-functions",
    "title": "Handwritten Digits Classification",
    "section": "",
    "text": "Here we will include all of the functions that we will use in this project.\n\n\nplot_digit &lt;- function(row) {\n  digit_mat &lt;-  row |&gt;\n    select(-digit)|&gt;\n    as.numeric()|&gt;\n    matrix(nrow = 28)\n  \n  image(digit_mat[,28:1])\n}\n\n\nplot_digit(): This function takes in a row of the ‘mnist’ dataset, removes the ‘digit’ column, turns the row into a vector using ‘as.numeric()’ function, and then turns it into a 28 by 28 matrix. Finally, it plots the matrix using the function ‘image()’.\n\n\nplot_region &lt;- function(tbl) {\n  digit_mat &lt;- as.matrix(tbl)*128 # Convert tbl into matrix and assign gray=128\n\n  image(t(digit_mat)[,28:1]) #Plot the image making sure is rotated\n}\n\n#plot_region(region1_class)\n\n\nplot_region(): produce a plot where the pixel of the region of interest is distinguish from the rest of the matrix. It does so through assigning the pixel value of all of the matrix as gray then using a different color for the region of interest. It also make sure that the plot is rotated correctly.\n\n\ncalc_prop &lt;- function (region, row) {\n  # Take row from mnist and transform into a \"digit\" matrix\n  digit_mat &lt;-  row |&gt;\n    as.numeric()|&gt;\n    matrix(nrow = 28) |&gt;\n    t()\n  # Find positions of pixels from \"region\"\n  pos = (region==1)\n  # Subset \"digit\" to the positions and count dark pixels (grey&gt;20)\n  dark = digit_mat[pos]&gt;20 \n  # Return proportion of dark pixels of \"image\" in \"region\"\n  return(sum(dark)/sum(pos))\n}\n\n\nThis function, calc_prop takes a region and a row as arguments and returns the proportion of dark pixels in each region. It does this by taking your dataset as a row and then calculating an proportion of “dark pixels” which it defines as a value of over 20 for grey."
  },
  {
    "objectID": "Digits classification.html#region-description",
    "href": "Digits classification.html#region-description",
    "title": "Handwritten Digits Classification",
    "section": "Region description",
    "text": "Region description\nHere we define the two regions that we will be using to predict the correct digit.\n\nReasons for using these two regions: Typically, the top and bottom of 4 have points and not many pixels in a horizontal line as opposed to 3 that has a lot. We made region2 curved because typically, a lot of 3s are more curved on the bottom than on the top. We didn’t choose the region in the middle because 3s and 4s often overlap a lot in the middle.\n\n\nregion1 &lt;- read_csv(\"~/Downloads/region1.csv\", col_names = FALSE)\nregion2 &lt;- read_csv(\"~/Downloads/region2.csv\", col_names = FALSE)\nplot_region(region1)\n\n\n\n\n\n\n\nplot_region(region2)"
  },
  {
    "objectID": "Digits classification.html#image-dataset",
    "href": "Digits classification.html#image-dataset",
    "title": "Handwritten Digits Classification",
    "section": "Image dataset",
    "text": "Image dataset\n\nHere we create our dataset by filtering mnist to include only our two digits of interest and 1000 observations.\n\n\nmnist3_4 &lt;- read_csv(\"~/Downloads/mnist.csv.gz\") |&gt;\n  mutate(digit=as.factor(digit)) |&gt;\n  filter(digit %in% c(3,4)) |&gt;\n  slice_head(n = 1000)"
  },
  {
    "objectID": "Digits classification.html#feature-dataset",
    "href": "Digits classification.html#feature-dataset",
    "title": "Handwritten Digits Classification",
    "section": "Feature dataset",
    "text": "Feature dataset\nIn this section, we have created a dataset called ‘features_tbl’ that includes four columns (id, digit, prop1, and prop2). Column ‘id’ contains consecutive number from 1 to 1000. ‘digit’ is a factor corresponding to digit 3 and digit 4. We calculated the proportion of dark pixels for ‘region1’ and saved the values in column ‘prop1’. We did the same calculation for ‘region2’ and saved the values under column ‘prop2’.\n\nfeatures_tbl &lt;- mnist3_4|&gt;\n  mutate(id = row_number()) |&gt;\n  rowwise()|&gt;\n  mutate(prop1 = calc_prop(region1,c_across(V1:V784)),\n         prop2 = calc_prop(region2,c_across(V1:V784)),\n         digit = as.factor(digit)) |&gt;\n  mutate(digit = fct_drop(digit)) |&gt;\n  ungroup()|&gt;\n  select(id, digit, prop1, prop2)"
  },
  {
    "objectID": "Digits classification.html#initial-plotting",
    "href": "Digits classification.html#initial-plotting",
    "title": "Handwritten Digits Classification",
    "section": "Initial plotting",
    "text": "Initial plotting\nHere we generate a plot of prop1 versus prop2 and use color to represent the two digit types.\n\nfeatures_tbl |&gt;\n  ggplot(aes(prop1, prop2, color = digit)) +\n  geom_jitter(alpha = 0.6)\n\n\n\n\n\n\n\n\n\nOur dataset looks pretty separate and distinguishable. There are 2 distinct clusters for 3 and 4. There are some outliers that mixed into the other cluster but not many."
  },
  {
    "objectID": "Digits classification.html#trainingtesting-definition",
    "href": "Digits classification.html#trainingtesting-definition",
    "title": "Handwritten Digits Classification",
    "section": "Training/Testing definition",
    "text": "Training/Testing definition\nHere, we use our features dataset (see section Feature Dataset) to create our training and testing datasets.\n\nWe first create an 80/20 split for our testing and training data sets.\n\n\nset.seed(12345)\nfeatures_split &lt;- initial_split(features_tbl, prop = .8)\nfeatures_train_tbl &lt;- training(features_split)\nfeatures_test_tbl &lt;- testing(features_split)"
  },
  {
    "objectID": "Digits classification.html#model-creation-optimization-and-selection",
    "href": "Digits classification.html#model-creation-optimization-and-selection",
    "title": "Handwritten Digits Classification",
    "section": "Model creation, optimization and selection",
    "text": "Model creation, optimization and selection\n\nWe next create a KNN model for distinguishing the two different digit types using our two defined features.\n\n\nuse_kknn(formula=digit~prop1+prop2, \n         data = features_train_tbl, \n         prefix = \"features\")\n\nWe then create our cross validation data set using the default value of 10 folds. Additionally we create a grid which contains both the values of k that we wish to test, along with the 4 different weight functions that we want to test in combination with our k values.\n\nset.seed(12345)\nfeatures_cv &lt;- vfold_cv(features_train_tbl)\n\nfeatures_neighbors &lt;- grid_regular(\n  neighbors(range = c(1, 50)),\n  weight_func(values = c(\"rectangular\", \"triangular\", \"inv\", \"cos\")),\n  levels = 10\n)\n\nNext, we create and tune our workflow for both our optimal k and our optimal workflow.\nWe decided to tune both of these parameters at the same time to make sure that there was not a scenario where the optimal k for one weight function was not the optimal k for another one.\n\nRectangular: This function is the most basic of all of them. It assigns all distances to the nearest neighbors a weight of 1, so all values are weighted equally, regardless of how far they are from the unknown point.\nTriangular: The neighbors are weighted based on their triangular distance. As you move away from the point of interest, the weight assigned to neighboring data points decreases.\nInverse: This weight function is somewhat similar in spirit to the triangular weight function, where neighbors closer to the unknown point are given a higher weight, but the way that it does this is somewhat different. With this function, values are given a weight which is \\(\\frac{1}{d}\\) where \\(d\\) is the distance that that point is from the unkown. In essence, values closer to the unknown get a higher weight than the ones that are farther away.\nCos: Similarly to triangular but the distance would be transform to \\(cos(d)\\) for weight. This makes closer points weighted more than point further away within a cosine curve\n\n\nfeatures_recipe &lt;- \n  recipe(formula = digit ~ prop1 + prop2, data = features_train_tbl) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) \n\nfeatures_spec &lt;- \n  nearest_neighbor(neighbors = tune(), weight_func = tune()) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"kknn\") \n\nfeatures_workflow &lt;- \n  workflow() |&gt;\n  add_recipe(features_recipe) |&gt;\n  add_model(features_spec) \n\nset.seed(12345)\nfeatures_tune &lt;-\n  tune_grid(features_workflow, \n            resamples = features_cv,\n            grid = features_neighbors)\n\nautoplot(features_tune, metric=\"accuracy\")\n\n\n\n\n\n\n\n\n\ncollect_metrics(features_tune) |&gt;\n  filter (.metric==\"accuracy\") |&gt;\n  slice_max(mean, n=5)\n\n# A tibble: 8 × 8\n  neighbors weight_func .metric  .estimator  mean     n std_err .config         \n      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1         6 rectangular accuracy binary     0.952    10 0.00667 pre0_mod07_post0\n2        17 triangular  accuracy binary     0.952    10 0.00717 pre0_mod16_post0\n3        17 inv         accuracy binary     0.951    10 0.00778 pre0_mod14_post0\n4        11 triangular  accuracy binary     0.951    10 0.00631 pre0_mod12_post0\n5        17 cos         accuracy binary     0.95     10 0.00791 pre0_mod13_post0\n6        28 inv         accuracy binary     0.95     10 0.00722 pre0_mod22_post0\n7        33 inv         accuracy binary     0.95     10 0.00722 pre0_mod26_post0\n8        39 inv         accuracy binary     0.95     10 0.00722 pre0_mod30_post0\n\n\nOur final conclusion shows that we have k optimized to 6 and the optimal weight function is the rectangular weight function.\nNext, we need to finalize our workflow and fit our model.\n\nshow_best(features_tune, metric=\"accuracy\")\n\n# A tibble: 5 × 8\n  neighbors weight_func .metric  .estimator  mean     n std_err .config         \n      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1         6 rectangular accuracy binary     0.952    10 0.00667 pre0_mod07_post0\n2        17 triangular  accuracy binary     0.952    10 0.00717 pre0_mod16_post0\n3        17 inv         accuracy binary     0.951    10 0.00778 pre0_mod14_post0\n4        11 triangular  accuracy binary     0.951    10 0.00631 pre0_mod12_post0\n5        17 cos         accuracy binary     0.95     10 0.00791 pre0_mod13_post0\n\n(opt_k_wfunc &lt;- select_best(features_tune, metric = \"accuracy\"))\n\n# A tibble: 1 × 3\n  neighbors weight_func .config         \n      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;           \n1         6 rectangular pre0_mod07_post0\n\nfeatures_knn_wf &lt;- finalize_workflow(features_workflow, opt_k_wfunc)\n\nfeatures_knn_model &lt;- fit(features_knn_wf, features_train_tbl)\n\nFinally, we can calculate our accuracy and view our confusion matrix to see how well our model works.\n\naugment(features_knn_model, features_test_tbl) |&gt;\n  metrics(.pred_class, digit)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.95 \n2 kap      binary         0.899\n\naugment(features_knn_model, features_test_tbl) |&gt;\n  conf_mat(truth = digit, estimate = .pred_class)\n\n          Truth\nPrediction   4   3\n         4 104  10\n         3   0  86\n\n\nOur final accuracy is 95% and we correctly classify 86 out of 96 3’s and 104 out of 104 4’s in the dataset."
  },
  {
    "objectID": "Digits classification.html#decision-boundary",
    "href": "Digits classification.html#decision-boundary",
    "title": "Handwritten Digits Classification",
    "section": "Decision boundary",
    "text": "Decision boundary\n\nHere we plot our model predictions decision boundary across a expand grid of prop1 and prop2\n\n\n(p1_vec = seq(0,1, by=0.01))\n(p2_vec = seq(0.0,1, by=0.01))\n(grid_tbl &lt;- expand_grid(prop1=p1_vec, prop2=p2_vec))\n\n\naugment(features_knn_model, grid_tbl) |&gt;\n  ggplot(aes(prop1, prop2, fill = .pred_class)) +\n  geom_raster() +\n  labs(title = \"Decision Boundary of Model Predictions\",\n       fill = \"Prediction Class\")\n\n\n\n\n\n\n\n\n\nHere we can see that the decision boundary of our model matches up very closely to the true values of 3’s and 4’s that we plotted earlier in this report."
  },
  {
    "objectID": "Digits classification.html#misclassifications",
    "href": "Digits classification.html#misclassifications",
    "title": "Handwritten Digits Classification",
    "section": "Misclassifications",
    "text": "Misclassifications\nWe are now interested in the few numbers that our model misclassified. Interestingly enough, our model had a 100% True Negative Rate, or in other words, it predicted that the true value was a 4 correctly every time. However, it did mess up a fair amount of the 3s, so we will now plot some of them and attempt to understand why the model misclassified them.\n\nmiss_class &lt;- augment(features_knn_model, features_test_tbl) |&gt; \n  filter(.pred_class != digit) \n\n# Plot 1\nplot_digit(mnist3_4[427,]) # 3 miss classified as 4\n\n\n\n\n\n\n\n# Plot 2\nplot_digit(mnist3_4[386,]) # 3 miss classified as 4\n\n\n\n\n\n\n\n\n\nWe plot two digits 3 misclassified as 4, we got a perfect 4 classifier, and this might be due to chance thanks to the similarity between the training and testing dataset.\n\nMiss Classification Reason:\nThese digits got misclassified because of unusual handwritings. In the first plot, the handwritten 3 is tilted, missing the top and some bottom regions. In the second plot, the handwritten digit misses pretty much the top region. The bad handwriting with angled orientation that might overlap the regions we selected in unexpected way\n\n# 3 miss classified as 4 proportions\nfeatures_tbl |&gt;  \n  filter(id %in% c(427,386)) |&gt;\n  select(id, prop1, prop2)\n\n# A tibble: 2 × 3\n     id prop1 prop2\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   386 0.233 0.487\n2   427 0.367 0.179\n\n\n\nThe above code shows the proportion values of region 1 and 2 for digit with id 386 and id 427.\n\n\n# Probability\nmiss_class |&gt;\n  filter(id %in% c(427,386)) |&gt;\n  select(id, .pred_class, .pred_3, .pred_4)\n\n# A tibble: 2 × 4\n     id .pred_class .pred_3 .pred_4\n  &lt;int&gt; &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1   386 4             0.333   0.667\n2   427 4             0       1    \n\n\n\nHere we see that the model gives the digit with id 386 a 1 in 3 chance of being a 3 and a 2 in 3 chance of being a 4, which is wrong, but also explains why the digit was misclassified. Similarly with the other digit, but even worse, the model gaave it a 100% chance of being a 4, even though it was a 3."
  },
  {
    "objectID": "Digits classification.html#changing-things-up",
    "href": "Digits classification.html#changing-things-up",
    "title": "Handwritten Digits Classification",
    "section": "Changing things up",
    "text": "Changing things up\n\nWe have pulled 500 digit 5s from mnist dataset, saved it under ‘mnist_5’ and then binded with the previous ‘mnist3_4’ dataset by rows. The resulting dataset is called ‘mnist3_4_5’.\n\n\nmnist &lt;- read_csv(\"~/Downloads/mnist.csv.gz\") \nmnist_5 &lt;- mnist|&gt;\n  mutate(digit=as.factor(digit)) |&gt;\n  filter(digit %in% c(5)) |&gt;\n  slice_head(n = 500)\n\nmnist3_4_5 &lt;- bind_rows(mnist3_4, mnist_5) # bind 2 datasets\n\n\nWe then calculated ‘prop1’ and ‘prop2’ on the new previous dataset like we did as the beginning of the project.\n\n\nmnist_3_4_5.features &lt;- mnist3_4_5|&gt;\n  mutate(id = row_number()) |&gt;\n  rowwise()|&gt;\n  mutate(prop1 = calc_prop(region1,c_across(V1:V784)),\n         prop2 = calc_prop(region2,c_across(V1:V784)),\n         digit = as.factor(digit)) |&gt;\n  mutate(digit = fct_drop(digit)) |&gt;\n  ungroup()|&gt;\n  select(id, digit, prop1, prop2)\n\n\nWe splitted the previous dataset into training/testing datasets that have 1200 and 300 observations respectively.\n\n\nset.seed(101325)\nmnist345_features_split &lt;- initial_split(mnist_3_4_5.features, prop = .8)\nmnist345_features_train &lt;- training(mnist345_features_split)\nmnist345_features_test &lt;- testing(mnist345_features_split)\n\n\nWe then re-trained the model using the new training dataset without optimizing parameters ‘k’ and ‘weight_func’.\n\n\nmnist345_features_recipe &lt;- \n  recipe(formula = digit ~ prop1 + prop2, data = mnist345_features_train)\n\nmnist345_features_spec &lt;- \n  nearest_neighbor() |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"kknn\") \n\nmnist345_features_workflow &lt;- \n  workflow() |&gt;\n  add_recipe(mnist345_features_recipe) |&gt;\n  add_model(mnist345_features_spec) \n\n\nBelow is the accuracy and confusion matrix of our retrained model.\n\n\nmnist345_model &lt;- fit(mnist345_features_workflow, mnist345_features_train)\n\n# Accuracy overall\naugment(mnist345_model, mnist345_features_test) |&gt;\n  metrics(.pred_class, digit)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.677\n2 kap      multiclass     0.515\n\n# Accuracy by each digit\naugment(mnist345_model, mnist345_features_test) |&gt;\n  group_by(digit) |&gt;\n  metrics(.pred_class, digit)\n\n# A tibble: 6 × 4\n  digit .metric  .estimator .estimate\n  &lt;fct&gt; &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 4     accuracy multiclass     0.860\n2 3     accuracy multiclass     0.694\n3 5     accuracy multiclass     0.485\n4 4     kap      multiclass     0    \n5 3     kap      multiclass     0    \n6 5     kap      multiclass     0    \n\naugment(mnist345_model, mnist345_features_test) |&gt;\n  conf_mat(.pred_class, digit)\n\n          Truth\nPrediction  4  3  5\n         4 80  4  9\n         3  6 75 27\n         5 20 31 48\n\n\nThe confusion matrix of the model shows that digit 3 and 5 get confused more with 35 and 33 miss classifications respectively. We see that 3 gets miss classified as 5 the most and 5 gets miss classified as 3 the most because digit 3 and 5 share similar regions that were chosen as region 1 and 2. The both overlap at the top and bottom regions.\n\nFinally, we plot the decision boundary for the retrained model.\n\n\naugment(mnist345_model, grid_tbl) |&gt;\n  ggplot(aes(prop1, prop2, fill = .pred_class)) +\n  geom_raster() +\n  labs(title = \"Decision Boundary of Model Predictions\",\n       subtitle = \"Digits 3,4,5\",\n       fill = \"Prediction Class\")\n\n\n\n\n\n\n\n\n\nWe can see here that the decision boundary here is a lot less clean, which makes sense, as we did not create our regions with the purpose of descriminating between digit 5 and our other numbers. However, there are still some regions that we can block off for each number."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Spotify song characteristics.html",
    "href": "Spotify song characteristics.html",
    "title": "Spotify Song Characteristics",
    "section": "",
    "text": "In this project, I will explore a dataset about Song characteristics from Spotify.\n\n\n\nlibrary(tidyverse)\nspotify &lt;- read_csv(\"~/Downloads/spotify.csv\")\n\n\nI will be using the “Spotify” data set. The data set has 200 observations with 155 unique songs collected from four points during the year 2021 along with the characteristic of each song. The original data was collected by two St. Olaf students as their project. It was collected using data from Spotify in order to understand why certain songs are popular.\n\n\nspotify|&gt;\n  distinct(title) |&gt;\n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   155\n\n\n\n\n\n\nExplore a quantitative response variable and binary categorical explanatory variable.\n\n\nI chose valence to be numeric response variable and instrumentalness to be binary categorical explanatory variable. Valence describes the musical positiveness of a track. The more positive a track is, the closer the value is to 1.0. Instrumentalness classifies whether a song is instrumental or not.\nResearch question: Is there a difference in the valence score of songs classified as instrumental and songs not classified as instrumentl? In other words, does instrumentalness affect a song’s valence score?\n\n\nExplore and describe the relationship between the two variables with appropriate summary statistics.\n\n\nmosaic::favstats(valence ~ instrumentalness, data = spotify)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n  instrumentalness    min      Q1 median      Q3   max      mean        sd   n\n1     instrumental 0.1000 0.28425 0.4225 0.57175 0.942 0.4495270 0.2273844  74\n2 not instrumental 0.0628 0.35700 0.4965 0.69800 0.934 0.5194762 0.2282755 126\n  missing\n1       0\n2       0\n\nfitlineA &lt;- lm(valence ~ instrumentalness, data = spotify)\nresid_panel(fitlineA)\n\n\n\n\n\n\n\n\n\nspotify |&gt;\n  ggplot(aes(y = valence, x = instrumentalness, fill = instrumentalness)) +\n  geom_boxplot(width = 0.25) +\n  geom_jitter(width = 0.05, alpha = 0.5) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Valence Score - Instrumental vs. Not Instrumental\",\n       x = \"Instrumentalness\",\n       y = \"Valence Score\") + coord_flip()\n\n\n\n\n\n\n\n\n\nInstrumental songs have mean valence score of 0.449, and non instrumental songs have mean valence score of 0.519. Not instrumental songs have a slightly higher mean valence score than instrumental songs.\n\n\nPerform the appropriate hypothesis test\n\nHypothesis Test\n\nNull hypothesis: \\(H_0: \\mu_{i} - \\mu_{ni} = 0\\). There is no difference in the mean valence score of instrumental and not instrumental songs. The instrumentalness doesn’t affect the songs’ valence score.\nAlternative hypothesis: \\(H_A: \\mu_{i} - \\mu_{ni} \\ne 0\\). There is a difference in the mean valence score of instrumental and not instrumental songs. The instrumentalness does affect the songs’ valence score.\n\n\nt.test(valence ~ instrumentalness, data = spotify)\n\n\n    Welch Two Sample t-test\n\ndata:  valence by instrumentalness\nt = -2.0974, df = 153.57, p-value = 0.0376\nalternative hypothesis: true difference in means between group instrumental and group not instrumental is not equal to 0\n95 percent confidence interval:\n -0.13583447 -0.00406386\nsample estimates:\n    mean in group instrumental mean in group not instrumental \n                     0.4495270                      0.5194762 \n\n\n\nFrom the t-test, we have the test statistic of -2.097 and p-value of 0.0376.\nThe 95% confidence interval is -0.136 and -0.004. We are 95% confident that mean valence scores for instrumental songs are between 0.004 and 0.136 points lower than non instrumental songs. There is no 0 within the interval so we know the difference is significant.\n\n\nCheck assumptions/conditions for the test.\n\n\nThere are two conditions for the test: Independent and Normality\nIndependent: Although the method of collecting the sample is not mentioned, we can safely assume that the observations are independent both within and between groups. Knowing one song’s valence score should not impact another song’s valence score.\nNormality: Both groups have sample sizes greater than 30, and there seems to be no big outliers, so normality is met.\n\n\nStatistical conclusion in context\n\n\nFrom the t-test, we have the test statistic of -2.097 and p-value of 0.0376. Because the p-value is less than 0.05, we reject the null hypothesis in favor of the alternative, and conclude that we have a statistically significant evidence that there is a difference the mean valence score of instrumental and not instrumental songs. The instrumentalness does affect the songs’ valence score. Under the null hypothesis, it is unlikely to see the difference in means that we did. The chance that p-value occurs as or more extreme is 3.76%. The test statistic also confirms this because it is more than two standard deviations away from the null.\n\n\nProvide an interpretation of the confidence interval in context\n\n\nThe 95% confidence interval is -0.136 and -0.004. We are 95% confident that mean valence scores for instrumental songs are between 0.004 and 0.136 points lower than non instrumental songs. There is no 0 within the interval so we know the difference is significant. Because valence describes the musical positiveness of a track, we can also say that instrumental songs are less positive than non instrumental songs by 0.004 to 0.136 points.\n\n\n\n\n\nExplore two binary categorical variables in the dataset.\n\n\nI chose top10 to be binary categorical response variable and mode to be binary categorical explanatory variable. top10 tells whether a song is ranked in the top 10 or not. mode indicates whether a track is in a major or minor key.\nResearch question: Is there a real difference in the proportion of top10 songs with major key compared to those with minor key?\n\n\nExplore and describe the relationship between the two variables with appropriate summary statistics.\n\n\n# Table of counts\ntable(spotify$mode, spotify$top10) |&gt; \n  addmargins()\n\n       \n         no yes Sum\n  major 105  22 127\n  minor  55  18  73\n  Sum   160  40 200\n\n# Table of proportions \ntable(spotify$mode, spotify$top10) |&gt;\n  proportions(margin = 1) |&gt; \n  round(3)\n\n       \n           no   yes\n  major 0.827 0.173\n  minor 0.753 0.247\n\n# Bar graph\nspotify |&gt;\nggplot(aes(x = mode, fill = top10)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Ranked in top 10 - Major vs. Minor\",\n       x = \"Mode\",\n       y = \"Proportion\",\n       fill = \"top10\")\n\n\n\n\n\n\n\n\n\nFrom the proportion table, we see that the proportion of top 10 ranked songs with major key is less than the proportion of top 10 ranked songs with minor key.\n\n\nPerform the appropriate hypothesis test\n\nHypothesis Test\n\nNull hypothesis: \\(H_0: p_{major} - p_{minor} = 0\\). There is no difference in the proportion of top10 songs with major key compared to those with minor key.\nAlternative: \\(H_A: p_{major} - p_{minor} \\ne 0\\). There is a difference in the proportion of top10 songs with major key compared to those with minor key.\n\n\nprop.test(x = c(22, 18), n = c(127, 73), conf.level = 0.95, \n          alternative = \"two.sided\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(22, 18) out of c(127, 73)\nX-squared = 1.1339, df = 1, p-value = 0.2869\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.20291093  0.05621694\nsample estimates:\n   prop 1    prop 2 \n0.1732283 0.2465753 \n\n\n\nsqrt(1.1339)\n\n[1] 1.064847\n\n\n\nThe test gives us the X-squared of 1.1339, taking the square root, we have the test statistic (z score) of 1.065. The p-value is 0.2869. The 95% confidence interval is between -0.203 and 0.056.\n\n\nCheck assumptions/conditions for the test\n\n\nThere are 2 conditions for the test: Independent and Normal (Success/Failure)\nIndependent: Although the method of collecting the sample is not mentioned, we can safely assume that the observations are independent both within and between groups. Knowing one song’s rank should not impact another song’s rank.\nNormal: We check for success (ranked in top 10) and failure (not in top 10) in each explanatory group. In the major group, there are 22 successes and 105 failures, both greater than 10. In the minor group, there are 18 successes and 55 failures, also greater than 10. Because there are at least 10 successes and failures in major and minor groups, the condition is met.\n\n\nStatistical conclusion in context\n\n\nWe have a z-score of 1.065 and p-value of 0.2869. Because z-score is less than two standard deviations from the null and p-value is greater than 0.05, we fail to reject the null hypothesis and conclude there is not significant evidence that there is a difference in proportion of top10 songs with major key compared to those with minor key. It is likely to have the p-value as or more extreme under the the null hypothesis with the chance of 28.69%.\n\n\nInterpretation of confidence interval\n\n\nThe 95% confidence interval is between -0.203 and 0.056. We are 95% confident that the proportion of top 10 songs with major keys is between 0.203 lower and 0.056 higher than the proportion of top 10 songs with minor keys. Because 0 is included in the interval, it indicates 0 is a plausible value for the difference. As with the hypothesis test, we conclude the difference is not significant.\n\n\n\n\n\nIdentify a question that can be answered with two categorical variables in the dataset. At least one of these variables will have more than two groups. Clearly state this question for a general audience, and identify the explanatory and response variable.\n\n\nI chose trend as categorical response variable and mode as categorical explanatory variable. Variable trend describes how a song moved in the rankings since the previous week (down, up, same, or new entry). Variable mode indicates whether a track is in a major or minor key.\nResearch question: Is there an association between genre and trend?\n\n\nExplore and describe the relationship between the two variables with appropriate summary statistics. Provide one plot and one sentence about the relationship (supported by summary stats).\n\n\n#Table of counts\ntable(spotify$mode, spotify$trend) |&gt;\n  addmargins()\n\n       \n        MOVE_DOWN MOVE_UP NEW_ENTRY SAME_POSITION Sum\n  major        56      43         4            24 127\n  minor        32      22         1            18  73\n  Sum          88      65         5            42 200\n\n#Table of proportions\ntable(spotify$mode, spotify$trend) %&gt;%\n  proportions(margin = 1) %&gt;%\n  round(3)\n\n       \n        MOVE_DOWN MOVE_UP NEW_ENTRY SAME_POSITION\n  major     0.441   0.339     0.031         0.189\n  minor     0.438   0.301     0.014         0.247\n\nspotify |&gt;\nggplot(aes(x = mode, fill = trend)) + \n  geom_bar(position = \"fill\")+\n  labs(x = \"Mode\", \n       y = \"Proportion\",\n       fill = \"Trend\",\n       title = \"Trend in rankings from previous week - Major vs. Minor \")\n\n\n\n\n\n\n\n\n\nFrom the EDA, it seems that there are less songs with major key staying in the same position of rankings since the previous week than songs with minor key.\n\n** I saw that table of counts have a column for “New_entry” songs. This means there are 5 songs that don’t have a ranking from previous week to have a comparison, so I decided to take out this column out. We have new table of counts, table of proportions and bar graph.\n\nspotify_new &lt;- spotify |&gt;\n  filter(trend != \"NEW_ENTRY\")\n\n\n#Table of counts\ntable(spotify_new$mode, spotify_new$trend) |&gt;\n  addmargins()\n\n       \n        MOVE_DOWN MOVE_UP SAME_POSITION Sum\n  major        56      43            24 123\n  minor        32      22            18  72\n  Sum          88      65            42 195\n\n#Table of proportions\ntable(spotify_new$mode, spotify_new$trend) %&gt;%\n  proportions(margin = 1) %&gt;%\n  round(3)\n\n       \n        MOVE_DOWN MOVE_UP SAME_POSITION\n  major     0.455   0.350         0.195\n  minor     0.444   0.306         0.250\n\nspotify_new |&gt;\nggplot(aes(x = mode, fill = trend)) + \n  geom_bar(position = \"fill\")+\n  labs(x = \"Mode\", \n       y = \"Proportion\",\n       fill = \"Trend\",\n       title = \"Trend in rankings from previous week - Major vs. Minor \")\n\n\n\n\n\n\n\n\n\nPerform the appropriate hypothesis test\n\nHypothesis Test\n\nNull hypothesis: \\(H_0:\\) There is no association between mode and trend.\nAlternative: \\(H_A:\\) There is an association between mode and trend.\n\n\ntable(spotify_new$mode, spotify_new$trend) %&gt;%\n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 0.91107, df = 2, p-value = 0.6341\n\n\n\nOur chi-square statistic from our data is 0.911. The p-value from the chi-square distribution with 2 degrees of freedom is 0.6341.\n\n\nCheck assumptions/conditions for the test.\n\n\nThere are 2 conditions for the test: Independent and Expected counts greater than 5.\nIndependent: Although the method of collecting the sample is not mentioned, we can safely assume that the observations are independent both within and between groups.\nExpected Counts: The conditions are met since the expected counts are over 5 for each cell.\n\n\ntable(spotify_new$mode, spotify_new$trend) %&gt;%\n  chisq.test() %&gt;%\n  .$expected\n\n       \n        MOVE_DOWN MOVE_UP SAME_POSITION\n  major  55.50769      41      26.49231\n  minor  32.49231      24      15.50769\n\n\n\nConclusion\n\n\nOur p-value is 0.6341, which is greater than 0.05. Because of that, we fail to reject the null hypothesis and conclude that there is no association between mode and trend, that these two variables are independent. Under the null hypothesis, it is very likely to observe the p-value as or more extreme with the chance of 63.41%. Therefore, we know that the mode of a song does not affect its rankings.\n\n\n\n\nSummary:\n\nBased on the analyses, we explored the relationships between song characteristics on Spotify such as mode, instrumentalness and valence score. First, we observed a statistically significant difference in the mean valence scores between instrumental and non-instrumental songs. Instrumental songs were found to be slightly less positive, with their valence scores estimated to be between 0.004 and 0.136 points lower, based on a 95% confidence interval. This finding suggests that instrumentalness does have a measurable effect on a song’s positiveness. Second, when comparing the proportion of top 10 songs in major keys versus minor keys, we did not find significant evidence of a difference. The results indicated that any variation could be due to chance, and the 95% confidence interval (-0.203 to 0.056) includes 0, reinforcing that the difference is not meaningful. Finally, we found that a song’s mode (major or minor) does not appear to influence its ranking trends. This conclusion is supported by the data, which showed no significant association, suggesting that mode and trend are independent. The second and third analyses have mode as explanatory variable, and in both cases, we fail to reject the null hypothesis, showing that the mode of a song doesn’t influence other characteristics of Spotify songs. While these conclusions are based on statistical evidence, it’s important to recognize potential limitations, such as the sample of songs in the dataset. Instead of picking the top 50 songs, we can randomly pick 50 songs throughout the four points in a year. This way can make sure that observations are indeed independent. We cannot generalize conclusions to a population (all Spotify songs) and make causal conclusions because it is not a random sample and there is no random assignment of explanatory variable.\n\n\nspotify |&gt;\n  ggplot(aes(y = valence, x = instrumentalness, fill = instrumentalness)) +\n  geom_boxplot(width = 0.25) +\n  geom_jitter(width = 0.05, alpha = 0.5) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Valence Score - Instrumental vs. Not Instrumental\",\n       x = \"Instrumentalness\",\n       y = \"Valence Score\") + coord_flip()\n\n\n\n\nThere is a statistically significant difference in the mean valence scores between instrumental and non-instrumental songs. Instrumental songs were found to be slightly less positive, with their valence scores estimated to be between 0.004 and 0.136 points lower, based on a 95% confidence interval. The data set features the top 50 songs on Spotify from 4 points during the year 2021 (Jan 1, Apr 1, July 1, Oct 1). Statistics performed by unpaired t-test with Welch correction. Sample size is 200, test statistics is -2.0974, and p-value is 0.0376."
  },
  {
    "objectID": "Spotify song characteristics.html#section-set-up",
    "href": "Spotify song characteristics.html#section-set-up",
    "title": "Spotify Song Characteristics",
    "section": "",
    "text": "library(tidyverse)\nspotify &lt;- read_csv(\"~/Downloads/spotify.csv\")\n\n\nI will be using the “Spotify” data set. The data set has 200 observations with 155 unique songs collected from four points during the year 2021 along with the characteristic of each song. The original data was collected by two St. Olaf students as their project. It was collected using data from Spotify in order to understand why certain songs are popular.\n\n\nspotify|&gt;\n  distinct(title) |&gt;\n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   155"
  },
  {
    "objectID": "Spotify song characteristics.html#section-a-two-means",
    "href": "Spotify song characteristics.html#section-a-two-means",
    "title": "Spotify Song Characteristics",
    "section": "",
    "text": "Explore a quantitative response variable and binary categorical explanatory variable.\n\n\nI chose valence to be numeric response variable and instrumentalness to be binary categorical explanatory variable. Valence describes the musical positiveness of a track. The more positive a track is, the closer the value is to 1.0. Instrumentalness classifies whether a song is instrumental or not.\nResearch question: Is there a difference in the valence score of songs classified as instrumental and songs not classified as instrumentl? In other words, does instrumentalness affect a song’s valence score?\n\n\nExplore and describe the relationship between the two variables with appropriate summary statistics.\n\n\nmosaic::favstats(valence ~ instrumentalness, data = spotify)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n  instrumentalness    min      Q1 median      Q3   max      mean        sd   n\n1     instrumental 0.1000 0.28425 0.4225 0.57175 0.942 0.4495270 0.2273844  74\n2 not instrumental 0.0628 0.35700 0.4965 0.69800 0.934 0.5194762 0.2282755 126\n  missing\n1       0\n2       0\n\nfitlineA &lt;- lm(valence ~ instrumentalness, data = spotify)\nresid_panel(fitlineA)\n\n\n\n\n\n\n\n\n\nspotify |&gt;\n  ggplot(aes(y = valence, x = instrumentalness, fill = instrumentalness)) +\n  geom_boxplot(width = 0.25) +\n  geom_jitter(width = 0.05, alpha = 0.5) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Valence Score - Instrumental vs. Not Instrumental\",\n       x = \"Instrumentalness\",\n       y = \"Valence Score\") + coord_flip()\n\n\n\n\n\n\n\n\n\nInstrumental songs have mean valence score of 0.449, and non instrumental songs have mean valence score of 0.519. Not instrumental songs have a slightly higher mean valence score than instrumental songs.\n\n\nPerform the appropriate hypothesis test\n\nHypothesis Test\n\nNull hypothesis: \\(H_0: \\mu_{i} - \\mu_{ni} = 0\\). There is no difference in the mean valence score of instrumental and not instrumental songs. The instrumentalness doesn’t affect the songs’ valence score.\nAlternative hypothesis: \\(H_A: \\mu_{i} - \\mu_{ni} \\ne 0\\). There is a difference in the mean valence score of instrumental and not instrumental songs. The instrumentalness does affect the songs’ valence score.\n\n\nt.test(valence ~ instrumentalness, data = spotify)\n\n\n    Welch Two Sample t-test\n\ndata:  valence by instrumentalness\nt = -2.0974, df = 153.57, p-value = 0.0376\nalternative hypothesis: true difference in means between group instrumental and group not instrumental is not equal to 0\n95 percent confidence interval:\n -0.13583447 -0.00406386\nsample estimates:\n    mean in group instrumental mean in group not instrumental \n                     0.4495270                      0.5194762 \n\n\n\nFrom the t-test, we have the test statistic of -2.097 and p-value of 0.0376.\nThe 95% confidence interval is -0.136 and -0.004. We are 95% confident that mean valence scores for instrumental songs are between 0.004 and 0.136 points lower than non instrumental songs. There is no 0 within the interval so we know the difference is significant.\n\n\nCheck assumptions/conditions for the test.\n\n\nThere are two conditions for the test: Independent and Normality\nIndependent: Although the method of collecting the sample is not mentioned, we can safely assume that the observations are independent both within and between groups. Knowing one song’s valence score should not impact another song’s valence score.\nNormality: Both groups have sample sizes greater than 30, and there seems to be no big outliers, so normality is met.\n\n\nStatistical conclusion in context\n\n\nFrom the t-test, we have the test statistic of -2.097 and p-value of 0.0376. Because the p-value is less than 0.05, we reject the null hypothesis in favor of the alternative, and conclude that we have a statistically significant evidence that there is a difference the mean valence score of instrumental and not instrumental songs. The instrumentalness does affect the songs’ valence score. Under the null hypothesis, it is unlikely to see the difference in means that we did. The chance that p-value occurs as or more extreme is 3.76%. The test statistic also confirms this because it is more than two standard deviations away from the null.\n\n\nProvide an interpretation of the confidence interval in context\n\n\nThe 95% confidence interval is -0.136 and -0.004. We are 95% confident that mean valence scores for instrumental songs are between 0.004 and 0.136 points lower than non instrumental songs. There is no 0 within the interval so we know the difference is significant. Because valence describes the musical positiveness of a track, we can also say that instrumental songs are less positive than non instrumental songs by 0.004 to 0.136 points."
  },
  {
    "objectID": "Spotify song characteristics.html#section-c-two-proportions",
    "href": "Spotify song characteristics.html#section-c-two-proportions",
    "title": "Spotify Song Characteristics",
    "section": "",
    "text": "Explore two binary categorical variables in the dataset.\n\n\nI chose top10 to be binary categorical response variable and mode to be binary categorical explanatory variable. top10 tells whether a song is ranked in the top 10 or not. mode indicates whether a track is in a major or minor key.\nResearch question: Is there a real difference in the proportion of top10 songs with major key compared to those with minor key?\n\n\nExplore and describe the relationship between the two variables with appropriate summary statistics.\n\n\n# Table of counts\ntable(spotify$mode, spotify$top10) |&gt; \n  addmargins()\n\n       \n         no yes Sum\n  major 105  22 127\n  minor  55  18  73\n  Sum   160  40 200\n\n# Table of proportions \ntable(spotify$mode, spotify$top10) |&gt;\n  proportions(margin = 1) |&gt; \n  round(3)\n\n       \n           no   yes\n  major 0.827 0.173\n  minor 0.753 0.247\n\n# Bar graph\nspotify |&gt;\nggplot(aes(x = mode, fill = top10)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Ranked in top 10 - Major vs. Minor\",\n       x = \"Mode\",\n       y = \"Proportion\",\n       fill = \"top10\")\n\n\n\n\n\n\n\n\n\nFrom the proportion table, we see that the proportion of top 10 ranked songs with major key is less than the proportion of top 10 ranked songs with minor key.\n\n\nPerform the appropriate hypothesis test\n\nHypothesis Test\n\nNull hypothesis: \\(H_0: p_{major} - p_{minor} = 0\\). There is no difference in the proportion of top10 songs with major key compared to those with minor key.\nAlternative: \\(H_A: p_{major} - p_{minor} \\ne 0\\). There is a difference in the proportion of top10 songs with major key compared to those with minor key.\n\n\nprop.test(x = c(22, 18), n = c(127, 73), conf.level = 0.95, \n          alternative = \"two.sided\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(22, 18) out of c(127, 73)\nX-squared = 1.1339, df = 1, p-value = 0.2869\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.20291093  0.05621694\nsample estimates:\n   prop 1    prop 2 \n0.1732283 0.2465753 \n\n\n\nsqrt(1.1339)\n\n[1] 1.064847\n\n\n\nThe test gives us the X-squared of 1.1339, taking the square root, we have the test statistic (z score) of 1.065. The p-value is 0.2869. The 95% confidence interval is between -0.203 and 0.056.\n\n\nCheck assumptions/conditions for the test\n\n\nThere are 2 conditions for the test: Independent and Normal (Success/Failure)\nIndependent: Although the method of collecting the sample is not mentioned, we can safely assume that the observations are independent both within and between groups. Knowing one song’s rank should not impact another song’s rank.\nNormal: We check for success (ranked in top 10) and failure (not in top 10) in each explanatory group. In the major group, there are 22 successes and 105 failures, both greater than 10. In the minor group, there are 18 successes and 55 failures, also greater than 10. Because there are at least 10 successes and failures in major and minor groups, the condition is met.\n\n\nStatistical conclusion in context\n\n\nWe have a z-score of 1.065 and p-value of 0.2869. Because z-score is less than two standard deviations from the null and p-value is greater than 0.05, we fail to reject the null hypothesis and conclude there is not significant evidence that there is a difference in proportion of top10 songs with major key compared to those with minor key. It is likely to have the p-value as or more extreme under the the null hypothesis with the chance of 28.69%.\n\n\nInterpretation of confidence interval\n\n\nThe 95% confidence interval is between -0.203 and 0.056. We are 95% confident that the proportion of top 10 songs with major keys is between 0.203 lower and 0.056 higher than the proportion of top 10 songs with minor keys. Because 0 is included in the interval, it indicates 0 is a plausible value for the difference. As with the hypothesis test, we conclude the difference is not significant."
  },
  {
    "objectID": "Spotify song characteristics.html#section-d-categorical-variables",
    "href": "Spotify song characteristics.html#section-d-categorical-variables",
    "title": "Spotify Song Characteristics",
    "section": "",
    "text": "Identify a question that can be answered with two categorical variables in the dataset. At least one of these variables will have more than two groups. Clearly state this question for a general audience, and identify the explanatory and response variable.\n\n\nI chose trend as categorical response variable and mode as categorical explanatory variable. Variable trend describes how a song moved in the rankings since the previous week (down, up, same, or new entry). Variable mode indicates whether a track is in a major or minor key.\nResearch question: Is there an association between genre and trend?\n\n\nExplore and describe the relationship between the two variables with appropriate summary statistics. Provide one plot and one sentence about the relationship (supported by summary stats).\n\n\n#Table of counts\ntable(spotify$mode, spotify$trend) |&gt;\n  addmargins()\n\n       \n        MOVE_DOWN MOVE_UP NEW_ENTRY SAME_POSITION Sum\n  major        56      43         4            24 127\n  minor        32      22         1            18  73\n  Sum          88      65         5            42 200\n\n#Table of proportions\ntable(spotify$mode, spotify$trend) %&gt;%\n  proportions(margin = 1) %&gt;%\n  round(3)\n\n       \n        MOVE_DOWN MOVE_UP NEW_ENTRY SAME_POSITION\n  major     0.441   0.339     0.031         0.189\n  minor     0.438   0.301     0.014         0.247\n\nspotify |&gt;\nggplot(aes(x = mode, fill = trend)) + \n  geom_bar(position = \"fill\")+\n  labs(x = \"Mode\", \n       y = \"Proportion\",\n       fill = \"Trend\",\n       title = \"Trend in rankings from previous week - Major vs. Minor \")\n\n\n\n\n\n\n\n\n\nFrom the EDA, it seems that there are less songs with major key staying in the same position of rankings since the previous week than songs with minor key.\n\n** I saw that table of counts have a column for “New_entry” songs. This means there are 5 songs that don’t have a ranking from previous week to have a comparison, so I decided to take out this column out. We have new table of counts, table of proportions and bar graph.\n\nspotify_new &lt;- spotify |&gt;\n  filter(trend != \"NEW_ENTRY\")\n\n\n#Table of counts\ntable(spotify_new$mode, spotify_new$trend) |&gt;\n  addmargins()\n\n       \n        MOVE_DOWN MOVE_UP SAME_POSITION Sum\n  major        56      43            24 123\n  minor        32      22            18  72\n  Sum          88      65            42 195\n\n#Table of proportions\ntable(spotify_new$mode, spotify_new$trend) %&gt;%\n  proportions(margin = 1) %&gt;%\n  round(3)\n\n       \n        MOVE_DOWN MOVE_UP SAME_POSITION\n  major     0.455   0.350         0.195\n  minor     0.444   0.306         0.250\n\nspotify_new |&gt;\nggplot(aes(x = mode, fill = trend)) + \n  geom_bar(position = \"fill\")+\n  labs(x = \"Mode\", \n       y = \"Proportion\",\n       fill = \"Trend\",\n       title = \"Trend in rankings from previous week - Major vs. Minor \")\n\n\n\n\n\n\n\n\n\nPerform the appropriate hypothesis test\n\nHypothesis Test\n\nNull hypothesis: \\(H_0:\\) There is no association between mode and trend.\nAlternative: \\(H_A:\\) There is an association between mode and trend.\n\n\ntable(spotify_new$mode, spotify_new$trend) %&gt;%\n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 0.91107, df = 2, p-value = 0.6341\n\n\n\nOur chi-square statistic from our data is 0.911. The p-value from the chi-square distribution with 2 degrees of freedom is 0.6341.\n\n\nCheck assumptions/conditions for the test.\n\n\nThere are 2 conditions for the test: Independent and Expected counts greater than 5.\nIndependent: Although the method of collecting the sample is not mentioned, we can safely assume that the observations are independent both within and between groups.\nExpected Counts: The conditions are met since the expected counts are over 5 for each cell.\n\n\ntable(spotify_new$mode, spotify_new$trend) %&gt;%\n  chisq.test() %&gt;%\n  .$expected\n\n       \n        MOVE_DOWN MOVE_UP SAME_POSITION\n  major  55.50769      41      26.49231\n  minor  32.49231      24      15.50769\n\n\n\nConclusion\n\n\nOur p-value is 0.6341, which is greater than 0.05. Because of that, we fail to reject the null hypothesis and conclude that there is no association between mode and trend, that these two variables are independent. Under the null hypothesis, it is very likely to observe the p-value as or more extreme with the chance of 63.41%. Therefore, we know that the mode of a song does not affect its rankings."
  },
  {
    "objectID": "Spotify song characteristics.html#section-e-conclusion-and-figure-caption",
    "href": "Spotify song characteristics.html#section-e-conclusion-and-figure-caption",
    "title": "Spotify Song Characteristics",
    "section": "",
    "text": "Summary:\n\nBased on the analyses, we explored the relationships between song characteristics on Spotify such as mode, instrumentalness and valence score. First, we observed a statistically significant difference in the mean valence scores between instrumental and non-instrumental songs. Instrumental songs were found to be slightly less positive, with their valence scores estimated to be between 0.004 and 0.136 points lower, based on a 95% confidence interval. This finding suggests that instrumentalness does have a measurable effect on a song’s positiveness. Second, when comparing the proportion of top 10 songs in major keys versus minor keys, we did not find significant evidence of a difference. The results indicated that any variation could be due to chance, and the 95% confidence interval (-0.203 to 0.056) includes 0, reinforcing that the difference is not meaningful. Finally, we found that a song’s mode (major or minor) does not appear to influence its ranking trends. This conclusion is supported by the data, which showed no significant association, suggesting that mode and trend are independent. The second and third analyses have mode as explanatory variable, and in both cases, we fail to reject the null hypothesis, showing that the mode of a song doesn’t influence other characteristics of Spotify songs. While these conclusions are based on statistical evidence, it’s important to recognize potential limitations, such as the sample of songs in the dataset. Instead of picking the top 50 songs, we can randomly pick 50 songs throughout the four points in a year. This way can make sure that observations are indeed independent. We cannot generalize conclusions to a population (all Spotify songs) and make causal conclusions because it is not a random sample and there is no random assignment of explanatory variable.\n\n\nspotify |&gt;\n  ggplot(aes(y = valence, x = instrumentalness, fill = instrumentalness)) +\n  geom_boxplot(width = 0.25) +\n  geom_jitter(width = 0.05, alpha = 0.5) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Valence Score - Instrumental vs. Not Instrumental\",\n       x = \"Instrumentalness\",\n       y = \"Valence Score\") + coord_flip()\n\n\n\n\nThere is a statistically significant difference in the mean valence scores between instrumental and non-instrumental songs. Instrumental songs were found to be slightly less positive, with their valence scores estimated to be between 0.004 and 0.136 points lower, based on a 95% confidence interval. The data set features the top 50 songs on Spotify from 4 points during the year 2021 (Jan 1, Apr 1, July 1, Oct 1). Statistics performed by unpaired t-test with Welch correction. Sample size is 200, test statistics is -2.0974, and p-value is 0.0376."
  }
]